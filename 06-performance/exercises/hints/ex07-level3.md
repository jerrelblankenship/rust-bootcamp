# Exercise 7 Hints - Level 3 (Complete Solutions)

## ðŸ”§ Complete solutions for blocking optimization...

### Checkpoint 1: Concurrent Operations
```rust
// COMPLETE SOLUTION:
let handles: Vec<_> = (0..10)
    .map(|i| thread::spawn(move || simulate_blocking_work(i)))
    .collect();
let results: Vec<u64> = handles.into_iter().map(|h| h.join().unwrap()).collect();
// 200ms â†’ 20ms (10x improvement through concurrency)
```

### Checkpoint 2: Batched Thread Operations  
```rust
// COMPLETE SOLUTION:
let chunk_size = 25;
let handles: Vec<_> = (0..100).step_by(chunk_size)
    .map(|start| {
        thread::spawn(move || {
            (start..start + chunk_size).map(|i| tiny_operation(i)).sum::<u64>()
        })
    })
    .collect();
let results: Vec<u64> = handles.into_iter().map(|h| h.join().unwrap()).collect();
// 100 threads â†’ 4 threads, much lower overhead
```

### Checkpoint 3: Concurrent I/O Operations
```rust
// COMPLETE SOLUTION:
let handles: Vec<_> = (0..20)
    .map(|i| thread::spawn(move || simulate_blocking_io(i)))
    .collect();
let results: Vec<usize> = handles.into_iter().map(|h| h.join().unwrap()).collect();
// 500ms â†’ 25ms (20x improvement through I/O overlap)
```

### Checkpoint 4: Pipeline Producer-Consumer
```rust
// COMPLETE SOLUTION:
use std::sync::mpsc;

let (tx, rx) = mpsc::channel();
let rx = Arc::new(Mutex::new(rx));

// Producer (unchanged)
let producer_handle = thread::spawn(move || {
    for i in 0..50 {
        thread::sleep(Duration::from_millis(2));
        tx.send(i).unwrap();
    }
});

// Multiple consumers for pipeline processing
let handles: Vec<_> = (0..4).map(|_| {
    let rx_clone = Arc::clone(&rx);
    thread::spawn(move || {
        let mut local_results = Vec::new();
        loop {
            let item = {
                let rx = rx_clone.lock().unwrap();
                rx.try_recv()
            };
            match item {
                Ok(value) => local_results.push(expensive_processing(value)),
                Err(_) => break,
            }
        }
        local_results
    })
}).collect();

producer_handle.join().unwrap();
let all_results: Vec<u64> = handles.into_iter()
    .flat_map(|h| h.join().unwrap())
    .collect();
// Processing overlaps with production
```

## ðŸŽ¯ Expected Performance Improvements
- Checkpoint 1: 200ms â†’ 20ms (10x faster through concurrency)
- Checkpoint 2: 30ms â†’ 8ms (4x faster, less thread overhead)
- Checkpoint 3: 500ms â†’ 25ms (20x faster through I/O overlap)
- Checkpoint 4: 150ms â†’ 60ms (2.5x faster through pipelining)

## ðŸŒŠ Concurrency Patterns Summary

| Pattern | Use Case | Benefit |
|---------|----------|---------|
| **Concurrent Execution** | Independent I/O operations | Overlap wait times |
| **Batched Threading** | Many small CPU tasks | Reduce thread overhead |
| **I/O Overlap** | File/network operations | Utilize wait time |
| **Pipeline Processing** | Producer-consumer streams | Continuous throughput |

The key insight: **Blocking is the enemy of performance!** Use concurrency to keep work flowing.